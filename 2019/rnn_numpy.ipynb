{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivation=False):\n",
    "    if not derivation:\n",
    "        return 1/(1+np.exp(-x))\n",
    "    else:\n",
    "        # (input need to be x = sigmoid(y))\n",
    "        return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2binary = {}\n",
    "binary_dim = 8\n",
    "\n",
    "largest_number = pow(2,binary_dim)\n",
    "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network:\n",
    "    def __init__(self):\n",
    "        self.layers = None\n",
    "        self.layers_list = []\n",
    "        self.root_layer = None\n",
    "        self.last_layer = None\n",
    "        \n",
    "        self.reste_weights = []\n",
    "        \n",
    "    def add_layer(self, layer_shape):\n",
    "        if(self.layers == None):\n",
    "            self.layers = layer(layer_shape)\n",
    "            self.root_layer = self.layers\n",
    "            self.layers.root = True\n",
    "        else:\n",
    "            new_layer = layer(layer_shape)\n",
    "            assert (self.layers.weight.shape[-1] == new_layer.weight.shape[0])\n",
    "            new_layer.back = self.layers\n",
    "            \n",
    "            self.layers.next = new_layer\n",
    "            self.layers = self.layers.next\n",
    "            \n",
    "        self.layers_list.append(self.layers)\n",
    "        self.reste_weights.append(deepcopy(self.layers))\n",
    "        self.last_layer = self.layers\n",
    "        \n",
    "    def forward(self, X, y):\n",
    "        current_layer = self.root_layer\n",
    "        prev_layer = None\n",
    "\n",
    "        last_activation = X   \n",
    "        while current_layer != None:\n",
    "            prev_layer = current_layer\n",
    "            \n",
    "            if not current_layer.not_reset:\n",
    "                current_layer.history = np.zeros(current_layer.weight.shape[1])\n",
    "                current_layer.activation_history = [np.zeros(current_layer.weight.shape[1])]\n",
    "\n",
    "                current_layer.future = np.zeros(current_layer.weight.shape[1])\n",
    "                current_layer.not_reset = True\n",
    "\n",
    "            accc = np.dot(last_activation, current_layer.weight)\n",
    "            current_layer.plain = accc\n",
    "            \n",
    "            if not current_layer.root and current_layer.next != None:\n",
    "                accc = current_layer.back.plain + np.dot(current_layer.history, current_layer.weight)\n",
    "                current_layer.plain = accc\n",
    "                current_layer.activation = sigmoid(accc)\n",
    "                current_layer.history = current_layer.activation\n",
    "            else:\n",
    "                current_layer.activation = sigmoid(accc)\n",
    "            \n",
    "            current_layer.activation_history.append(deepcopy(current_layer.activation))\n",
    "                            \n",
    "            last_activation = current_layer.activation\n",
    "            current_layer = current_layer.next    \n",
    "    \n",
    "        error = y - prev_layer.activation\n",
    "        \n",
    "        delta = (error) * sigmoid(prev_layer.activation, derivation=True)\n",
    "        \n",
    "        prev_layer.delta_history.append(delta)\n",
    "        \n",
    "        return error, delta \n",
    "       \n",
    "    def backprop(self, index, hint):\n",
    "        current_layer = self.last_layer\n",
    "        current_layer = current_layer.back\n",
    "        \n",
    "        current_layer.next.update += np.atleast_2d(current_layer.activation_history[-index - 1]).T.dot(current_layer.next.delta_history[-index-1])\n",
    "        while current_layer.back != None:\n",
    "            if(current_layer.next.next == None):\n",
    "                current_layer.delta = (current_layer.future.dot(current_layer.weight.T) + current_layer.next.delta_history[-index - 1].dot(current_layer.next.weight.T))  * \\\n",
    "                sigmoid(current_layer.activation_history[-index - 1], derivation=True)\n",
    "            else:\n",
    "                current_layer.delta = (current_layer.future.dot(current_layer.weight.T) + current_layer.next.delta.dot(current_layer.next.weight.T))  * \\\n",
    "                sigmoid(current_layer.activation_history[-index - 1], derivation=True)\n",
    "              \n",
    "            current_layer.future = current_layer.delta\n",
    "            current_layer.update += np.atleast_2d(current_layer.activation_history[-index - 2]).T.dot(current_layer.delta)                        \n",
    "            \n",
    "            current_layer.not_reset = False\n",
    "            current_layer = current_layer.back\n",
    "            \n",
    "        current_layer.update += hint.T.dot(current_layer.next.delta)\n",
    "\n",
    "    def update_weights(self, lr=0.1):\n",
    "        current_layer = self.root_layer\n",
    "        while current_layer != None:\n",
    "            current_layer.weight += current_layer.update * lr\n",
    "            current_layer.update = np.zeros_like(current_layer.weight)\n",
    "            current_layer = current_layer.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "    def __init__(self, shape):\n",
    "        self.weight = 2 * np.random.random(shape) - 1\n",
    "        self.activation = None\n",
    "        self.next = None\n",
    "        self.back = None\n",
    "        \n",
    "        self.activation_history = []\n",
    "        self.delta_history = []\n",
    "        self.future = None\n",
    "        \n",
    "        self.update = np.zeros_like(self.weight)\n",
    "        self.not_reset = False\n",
    "        \n",
    "        self.root = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = network()\n",
    "model.add_layer((2, 16))\n",
    "model.add_layer((16, 16))\n",
    "model.add_layer((16, 16))\n",
    "model.add_layer((16, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_number(max_number):\n",
    "    global int2binary\n",
    "    index = np.random.randint(max_number//2)\n",
    "    return index, int2binary[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:[3.91222995]\n",
      "Error:[3.9807459]\n",
      "Error:[3.7766781]\n",
      "Error:[3.8955472]\n",
      "Error:[3.93633543]\n",
      "Error:[3.9391484]\n",
      "Error:[3.76891634]\n",
      "Error:[3.15148586]\n",
      "Error:[2.89824718]\n",
      "Error:[1.60611771]\n",
      "Error:[2.4416812]\n",
      "Error:[2.04229347]\n",
      "Error:[2.8324819]\n",
      "Error:[0.87739575]\n",
      "Error:[1.20450642]\n",
      "Error:[0.97796444]\n",
      "Error:[0.99277245]\n",
      "Error:[0.28491034]\n",
      "Error:[0.72561832]\n",
      "Error:[0.54280085]\n",
      "Error:[0.56233072]\n",
      "Error:[0.30364332]\n",
      "Error:[0.41061162]\n",
      "Error:[0.29634781]\n",
      "Error:[0.10989977]\n",
      "Error:[0.74120663]\n",
      "Error:[0.26088008]\n",
      "Error:[0.07895541]\n",
      "Error:[0.62199828]\n",
      "Error:[0.09463485]\n",
      "Error:[0.33909516]\n",
      "Error:[0.17349216]\n",
      "Error:[1.38725325]\n",
      "Error:[0.20859493]\n",
      "Error:[0.24593607]\n",
      "Error:[0.14610022]\n",
      "Error:[0.23899025]\n",
      "Error:[0.52393117]\n",
      "Error:[0.28595834]\n",
      "Error:[0.5736941]\n",
      "Error:[0.20088822]\n",
      "Error:[0.29609806]\n",
      "Error:[0.09263957]\n",
      "Error:[0.07650281]\n",
      "Error:[0.80191915]\n",
      "Error:[0.13564556]\n",
      "Error:[0.17952487]\n",
      "Error:[0.15397945]\n",
      "Error:[0.15549367]\n",
      "Error:[0.41394396]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "for j in range(50000):\n",
    "    number_a, binary_a = get_random_number(largest_number)\n",
    "    number_b, binary_b = get_random_number(largest_number)\n",
    "    number_c = number_a + number_b\n",
    "    binary_c = int2binary[number_c]\n",
    "    \n",
    "    number_a, binary_a = get_random_number(largest_number)\n",
    "    number_b, binary_b = get_random_number(largest_number)\n",
    "    number_c = number_a + number_b\n",
    "    binary_c = int2binary[number_c]\n",
    "    \n",
    "    predicted_d = np.zeros_like(binary_c)\n",
    "    \n",
    "    model.error = 0   \n",
    "    for position in range(binary_dim):   \n",
    "        X = np.array([[binary_a[binary_dim - position - 1], binary_b[binary_dim - position - 1]]])\n",
    "        y = np.array([[binary_c[binary_dim - position - 1]]]).T\n",
    "\n",
    "        error, delta = model.forward(X, y)\n",
    "        model.error += np.abs(error[0])    \n",
    "      \n",
    "        predicted_d[binary_dim - position - 1] = np.round(model.layers_list[-2].activation[0][0])\n",
    "           \n",
    "    for position in range(binary_dim):\n",
    "        X = np.array([[binary_a[position], binary_b[position]]])        \n",
    "        model.backprop(position, X)\n",
    "    \n",
    "    model.update_weights()\n",
    "\n",
    "    if(j % 1000 == 0):\n",
    "        print(\"Error:\" + str(model.error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
